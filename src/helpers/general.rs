use reqwest::{Client, Response};
use serde::de::DeserializeOwned;

use crate::apis::call_request::call_gpt;
use crate::helpers::command_line::PrintCommand;
use crate::models::agent_basic::basic_agent::BasicAgent;
use crate::models::agents::agent_backend::AgentBackendDeveloper;
use crate::models::agents::agent_traits::{FactSheet, SpecialFunctions};
use crate::models::general::llm::Message;

/// Takes in both the string version of an AI function
/// Combines this with the user input to encourage a structured printout in a program-like response
pub fn extend_ai_function(ai_func: fn(&str) -> &'static str, func_input: &str) -> Message {
    // Extract AI function text
    let ai_function_str: &str = ai_func(func_input);

    // Combine and AI function as string with Instruction
    let msg: String = format!("FUNCTION: {}
INSTRUCTION: You are a function printer. You only print the results of functions. Nothing else. No commentary.
Here is the input to the function: '{}'. Print out what the function will return.",
    ai_function_str, func_input);

    // Return result in Message format
    Message {
        role: "system".to_string(),
        content: msg,
    }
}

// performs call to LLM GPT
// Also makes a reattempt when he first call fails
// msg_context: is going to be our func_input
// agent_pos: architect, backend or managing
// agent_operation: what is the agent working on
// function_pass: what function are we actually passing
//
pub async fn ai_task_request(
    msg_context: String,
    agent_position: &str,
    agent_operation: &str,
    function_pass: for<'a> fn(&'a str) -> &'static str,
) -> String {
    // Extend AI function
    let extended_msg: Message = extend_ai_function(function_pass, &msg_context);

    // print current status
    PrintCommand::AICall.print_agent_message(agent_position, agent_operation);

    // Get LLM Response
    let llm_response: Result<String, Box<dyn std::error::Error + Send>> =
        call_gpt(vec![extended_msg.clone()]).await;

    dbg!(&llm_response);

    // Return success or try again
    match llm_response {
        Ok(llm_resp) => llm_resp,
        Err(_) => call_gpt(vec![extended_msg.clone()])
            .await
            .expect("Failed twice to call OpenAI"),
    }
}

pub async fn ai_task_request_decoded<T: DeserializeOwned>(
    msg_context: String,
    agent_position: &str,
    agent_operation: &str,
    function_pass: for<'a> fn(&'a str) -> &'static str,
) -> T {
    let llm_response =
        ai_task_request(msg_context, agent_position, agent_operation, function_pass).await;

    // dbg!(llm_response.as_str());

    let decoded_response: T =
        serde_json::from_str(llm_response.as_str()).expect("Failed to decode ai response");
    decoded_response
}

// Check whether urls included are valid
pub async fn check_status_code(client: &Client, url: &str) -> Result<u16, reqwest::Error> {
    let response: Response = client.get(url).send().await?;
    Ok(response.status().as_u16())
}

// The following contains the template code for the reference of LLM.
const CODE_TEMPLATE_PATH: &str =
    "/Users/neeraj/Projects/Practice/Rust/Udemy-Rust-AutoGippity/rust_auto_gpt/web_template/src/code_template.rs";

// The following stores the code generated by LLM. It is inside the auto_gippity
const GENERATED_PROJECT_PATH: &str =
    "/Users/neeraj/Projects/Practice/Rust/Udemy-Rust-AutoGippity/rust_auto_gpt/auto_gippity/web_app_generated/";

const EXEC_MAIN_PATH: &str =
    "/Users/neeraj/Projects/Practice/Rust/Udemy-Rust-AutoGippity/rust_auto_gpt/auto_gippity/web_app_generated/src/main.rs";

const API_SCHEMA_PATH: &str =
    "/Users/neeraj/Projects/Practice/Rust/Udemy-Rust-AutoGippity/rust_auto_gpt/auto_gippity/web_app_generated/schemas/api_schemas.json";
// Get Generated Project Path
pub fn get_generated_project_path() -> String {
    return GENERATED_PROJECT_PATH.to_string();
}

// Get Code Template
pub fn read_code_template_contents() -> String {
    let path: String = String::from(CODE_TEMPLATE_PATH);
    std::fs::read_to_string(path).expect("Failed to read code template")
}

// Get Executable Code generated by the LLM
pub fn read_exec_main_contents() -> String {
    let path: String = String::from(EXEC_MAIN_PATH);
    std::fs::read_to_string(path).expect("Failed to read main code generated by LLM")
}

// Save New Backend Code
pub fn save_backend_code(contents: &String) {
    let path: String = String::from(EXEC_MAIN_PATH);
    std::fs::write(path, contents).expect("Failed to write main.rs code");
}

// Save JSON API Endpoint Schema
pub fn save_api_endpoints(api_endpoints: &String) {
    let path: String = String::from(API_SCHEMA_PATH);
    std::fs::write(path, api_endpoints).expect("Failed to write api_endpoints to file");
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::ai_functions::aifunc_managing::convert_user_input_to_goal;

    #[test]
    fn tests_extending_ai_function() {
        let extended_msg = extend_ai_function(convert_user_input_to_goal, "dummy variable");
        // println!("Exended Message:\n{}", extended_msg.content);
        dbg!(&extended_msg);

        assert_eq!(extended_msg.role, "system".to_string());
    }

    #[tokio::test]
    async fn tests_ai_task_request() {
        // let ai_func_param = "Build me a website that makes pink elephants fly";
        let ai_func_param = "Build me a webserver for making stock price api requests".to_string();

        let res = ai_task_request(
            ai_func_param,
            "Managing Agent",
            "Defining User Requirements",
            convert_user_input_to_goal,
        )
        .await;

        println!("Response:\n{}", res);
        assert!(res.len() > 20);
    }
}
